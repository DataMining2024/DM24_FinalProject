{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T05:39:09.578775Z","iopub.status.busy":"2024-05-08T05:39:09.578403Z","iopub.status.idle":"2024-05-08T05:39:09.586280Z","shell.execute_reply":"2024-05-08T05:39:09.585353Z","shell.execute_reply.started":"2024-05-08T05:39:09.578744Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import imageio.v3 as imageio\n","import albumentations as A\n","\n","from albumentations.pytorch import ToTensorV2\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torch import nn\n","from tqdm.notebook import tqdm\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestRegressor\n","\n","import torch\n","import timm\n","import glob\n","import torchmetrics\n","import time\n","import psutil\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(torch.cuda.is_available())\n","print(torch.cuda.device_count())\n","print(torch.cuda.current_device())\n","print(torch.cuda.get_device_name(0))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T05:39:10.906631Z","iopub.status.busy":"2024-05-08T05:39:10.906249Z","iopub.status.idle":"2024-05-08T05:39:10.912867Z","shell.execute_reply":"2024-05-08T05:39:10.911938Z","shell.execute_reply.started":"2024-05-08T05:39:10.906600Z"},"trusted":true},"outputs":[],"source":["class Config():\n","    IMAGE_SIZE = 384\n","    BACKBONE = 'swin_large_patch4_window12_384.ms_in22k_ft_in1k'\n","    TARGET_COLUMNS = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n","    N_TARGETS = len(TARGET_COLUMNS)\n","    BATCH_SIZE = 16\n","    LR_MAX = 1e-4 \n","    WEIGHT_DECAY = 0.02\n","    N_EPOCHS = 40\n","    TRAIN_MODEL = True\n","    IS_INTERACTIVE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Interactive') == 'Interactive'\n","        \n","CONFIG = Config()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T05:39:13.858232Z","iopub.status.busy":"2024-05-08T05:39:13.857518Z","iopub.status.idle":"2024-05-08T05:41:47.604535Z","shell.execute_reply":"2024-05-08T05:41:47.603261Z","shell.execute_reply.started":"2024-05-08T05:39:13.858175Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv('/mnt/ljh/planttraits2024/updated_train.csv')\n","train['file_path'] = train['id'].apply(lambda s: f'/mnt/ljh/planttraits2024/train_images/{s}.jpeg')\n","train['jpeg_bytes'] = train['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\n","train.to_pickle('train.pkl')\n","\n","for column in CONFIG.TARGET_COLUMNS:\n","    lower_quantile = train[column].quantile(0.005)\n","    upper_quantile = train[column].quantile(0.985)  \n","    train = train[(train[column] >= lower_quantile) & (train[column] <= upper_quantile)]\n","\n","# RandomForest를 활용하여  각 target column별로 중요한 feature 추출 +  train set에 추가\n","important_features = {}\n","for target in CONFIG.TARGET_COLUMNS:\n","    y = train[target]\n","    X = train.drop(columns=CONFIG.TARGET_COLUMNS + ['id', 'file_path', 'jpeg_bytes'])\n","    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n","    model.fit(X, y)\n","    feature_importances = model.feature_importances_\n","    features_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importances})\n","    features_df = features_df.sort_values(by='importance', ascending=False)\n","    important_features[target] = features_df['feature'].values[:20] \n","\n","print(important_features)\n","\n","selected_features = list(set([feature for features in important_features.values() for feature in features]))\n","selected_feature_data = train[selected_features]\n","\n","train = pd.concat([train, selected_feature_data], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CONFIG.N_TRAIN_SAMPLES = len(train)\n","CONFIG.N_STEPS_PER_EPOCH = (CONFIG.N_TRAIN_SAMPLES // CONFIG.BATCH_SIZE)\n","CONFIG.N_STEPS = CONFIG.N_STEPS_PER_EPOCH * CONFIG.N_EPOCHS + 1\n","\n","test = pd.read_csv('/mnt/ljh/planttraits2024/test.csv')\n","test['file_path'] = test['id'].apply(lambda s: f'/mnt/ljh/planttraits2024/test_images/{s}.jpeg')\n","test['jpeg_bytes'] = test['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\n","test.to_pickle('test.pkl')\n","\n","print('N_TRAIN_SAMPLES:', len(train), 'N_TEST_SAMPLES:', len(test))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T05:41:47.607958Z","iopub.status.busy":"2024-05-08T05:41:47.607229Z","iopub.status.idle":"2024-05-08T05:41:47.629809Z","shell.execute_reply":"2024-05-08T05:41:47.629071Z","shell.execute_reply.started":"2024-05-08T05:41:47.607919Z"},"trusted":true},"outputs":[],"source":["LOG_FEATURES = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n","\n","y_train = np.zeros_like(train[CONFIG.TARGET_COLUMNS], dtype=np.float32)\n","for target_idx, target in enumerate(CONFIG.TARGET_COLUMNS):\n","    v = train[target].values\n","    if target in LOG_FEATURES:\n","        v = np.log10(v)\n","    y_train[:, target_idx] = v\n","\n","SCALER = StandardScaler()\n","y_train = SCALER.fit_transform(y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T05:41:47.631291Z","iopub.status.busy":"2024-05-08T05:41:47.630942Z","iopub.status.idle":"2024-05-08T05:41:48.302441Z","shell.execute_reply":"2024-05-08T05:41:48.301550Z","shell.execute_reply.started":"2024-05-08T05:41:47.631265Z"},"trusted":true},"outputs":[],"source":["MEAN = [0.485, 0.456, 0.406]  # NumPy 배열 대신 리스트 사용\n","STD = [0.229, 0.224, 0.225] \n","TRAIN_TRANSFORMS = A.Compose([\n","        A.HorizontalFlip(p=0.5),\n","        A.RandomSizedCrop(\n","            [448, 512],\n","            CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE, w2h_ratio=1.0, p=0.75),\n","        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n","        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.25),\n","        A.ImageCompression(quality_lower=85, quality_upper=100, p=0.25),\n","        A.ToFloat(),\n","        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n","        ToTensorV2(),\n","    ])\n","\n","TEST_TRANSFORMS = A.Compose([\n","        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n","        A.ToFloat(),\n","        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n","        ToTensorV2(),\n","    ])\n","\n","class Dataset(Dataset):\n","    def __init__(self, X_jpeg_bytes, y, transforms=None):\n","        self.X_jpeg_bytes = X_jpeg_bytes\n","        self.y = y\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.X_jpeg_bytes)\n","\n","    def __getitem__(self, index):\n","        X_sample = self.transforms(\n","            image=imageio.imread(self.X_jpeg_bytes[index]),\n","        )['image']\n","        y_sample = self.y[index]\n","        \n","        return X_sample, y_sample\n","\n","train_dataset = Dataset(\n","    train['jpeg_bytes'].values,\n","    y_train,\n","    TRAIN_TRANSFORMS,\n",")\n","\n","train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=CONFIG.BATCH_SIZE,\n","        shuffle=True,\n","        drop_last=True,\n","        num_workers=psutil.cpu_count(),\n",")\n","\n","test_dataset = Dataset(\n","    test['jpeg_bytes'].values,\n","    test['id'].values,\n","    TEST_TRANSFORMS,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T05:41:48.320104Z","iopub.status.busy":"2024-05-08T05:41:48.319769Z","iopub.status.idle":"2024-05-08T05:41:53.197998Z","shell.execute_reply":"2024-05-08T05:41:53.197070Z","shell.execute_reply.started":"2024-05-08T05:41:48.320076Z"},"trusted":true},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = timm.create_model(\n","                CONFIG.BACKBONE,\n","                num_classes=CONFIG.N_TARGETS,\n","                pretrained=True)\n","        \n","    def forward(self, inputs):\n","        return self.backbone(inputs)\n","\n","model = Model()\n","if torch.cuda.device_count() > 1:\n","    print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n","    model = nn.DataParallel(model)\n","\n","model = model.to('cuda')\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T05:41:53.199496Z","iopub.status.busy":"2024-05-08T05:41:53.199163Z","iopub.status.idle":"2024-05-08T05:41:53.486013Z","shell.execute_reply":"2024-05-08T05:41:53.485060Z","shell.execute_reply.started":"2024-05-08T05:41:53.199469Z"},"trusted":true},"outputs":[],"source":["def get_lr_scheduler(optimizer):\n","    return torch.optim.lr_scheduler.OneCycleLR(\n","        optimizer=optimizer,\n","        max_lr=CONFIG.LR_MAX,\n","        total_steps=CONFIG.N_STEPS,\n","        pct_start=0.1,\n","        anneal_strategy='cos',\n","        div_factor=1e1,\n","        final_div_factor=1e1,\n","    )\n","\n","class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val):\n","        self.sum += val.sum()\n","        self.count += val.numel()\n","        self.avg = self.sum / self.count\n","\n","MAE = torchmetrics.regression.MeanAbsoluteError().to('cuda')\n","R2 = torchmetrics.regression.R2Score(num_outputs=CONFIG.N_TARGETS, multioutput='uniform_average').to('cuda')\n","LOSS = AverageMeter()\n","\n","Y_MEAN = torch.tensor(y_train).mean(dim=0).to('cuda')\n","EPS = torch.tensor([1e-6]).to('cuda')\n","\n","def r2_loss(y_pred, y_true):\n","    ss_res = torch.sum((y_true - y_pred)**2, dim=0)\n","    ss_total = torch.sum((y_true - Y_MEAN)**2, dim=0)\n","    ss_total = torch.maximum(ss_total, EPS)\n","    r2 = torch.mean(ss_res / ss_total)\n","    return r2\n","\n","optimizer = torch.optim.AdamW(\n","    params=model.parameters(),\n","    lr=CONFIG.LR_MAX,\n","    weight_decay=CONFIG.WEIGHT_DECAY,\n",")\n","\n","LR_SCHEDULER = get_lr_scheduler(optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def weighted_smooth_l1_loss(y_pred, y_true, weights, beta=1.0):\n","    diff = torch.abs(y_pred - y_true)\n","    loss = torch.where(diff < beta, 0.5 * diff**2 / beta, diff - 0.5 * beta)\n","    weighted_loss = (weights * loss).mean()\n","    return weighted_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T05:41:53.487864Z","iopub.status.busy":"2024-05-08T05:41:53.487578Z"},"trusted":true},"outputs":[],"source":["weights = torch.tensor([0.17, 0.17, 0.13, 0.21, 0.16, 0.16], device='cuda')\n","best_loss = float('inf')\n","early_stopping_counter = 0\n","best_mae = float('inf')\n","\n","print(\"Start Training:\")\n","best_loss = float('inf')\n","for epoch in range(CONFIG.N_EPOCHS):\n","    MAE.reset()\n","    R2.reset()\n","    LOSS.reset()\n","    model.train()\n","        \n","    for step, (X_batch, y_true) in enumerate(train_dataloader):\n","        X_batch = X_batch.to('cuda')\n","        y_true = y_true.to('cuda')\n","        t_start = time.perf_counter_ns()\n","        y_pred = model(X_batch)\n","        loss = weighted_smooth_l1_loss(y_pred, y_true, weights)\n","        LOSS.update(loss)\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        LR_SCHEDULER.step()\n","        MAE.update(y_pred, y_true)\n","        R2.update(y_pred, y_true)\n","            \n","        if not CONFIG.IS_INTERACTIVE and (step+1) == CONFIG.N_STEPS_PER_EPOCH:\n","            print(\n","                f'EPOCH {epoch+1:02d}, {step+1:04d}/{CONFIG.N_STEPS_PER_EPOCH} | ' + \n","                f'loss: {LOSS.avg:.4f}, mae: {MAE.compute().item():.4f}, r2: {R2.compute().item():.4f}, ' +\n","                f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n","            )\n","        elif CONFIG.IS_INTERACTIVE:\n","            print(\n","                f'\\rEPOCH {epoch+1:02d}, {step+1:04d}/{CONFIG.N_STEPS_PER_EPOCH} | ' + \n","                f'loss: {LOSS.avg:.4f}, mae: {MAE.compute().item():.4f}, r2: {R2.compute().item():.4f}, ' +\n","                f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n","                end='\\n' if (step + 1) == CONFIG.N_STEPS_PER_EPOCH else '', flush=True,\n","            )\n","\n","    if best_loss > LOSS.avg + 0.001:\n","        if best_mae > MAE.compute().item():\n","            best_mae = MAE.compute().item()\n","        best_loss = LOSS.avg\n","        early_stopping_counter = 0\n","        torch.save(model, 'best_model.pth')\n","        print(\"Best Model!\")\n","    else:\n","        if best_mae > MAE.compute().item() + 0.005:\n","            best_mae = MAE.compute().item()\n","            torch.save(model, 'best_model.pth')\n","            print(\"Best Model!\")\n","            early_stopping_counter = 0\n","        else :\n","            early_stopping_counter += 1\n","            if early_stopping_counter >= CONFIG.EARLY_STOPPING_PATIENCE:\n","                print(\"Early stopping\")\n","                break\n","        \n","torch.save(model, 'model.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["SUBMISSION_ROWS = []\n","model = torch.load('best_model.pth')\n","model.eval()\n","\n","for X_sample_test, test_id in tqdm(test_dataset):\n","    with torch.no_grad():\n","        y_pred = model(X_sample_test.unsqueeze(0).to('cuda')).detach().cpu().numpy()\n","    \n","    y_pred = SCALER.inverse_transform(y_pred).squeeze()\n","    row = {'id': test_id}\n","    \n","    for k, v in zip(CONFIG.TARGET_COLUMNS, y_pred):\n","        if k in LOG_FEATURES:\n","            row[k.replace('_mean', '')] = 10 ** v\n","        else:\n","            row[k.replace('_mean', '')] = v\n","\n","    SUBMISSION_ROWS.append(row)\n","    \n","submission_df = pd.DataFrame(SUBMISSION_ROWS)\n","submission_df.to_csv('/home/leejaehoon/submission.csv', index=False)\n","print(\"Submit!\")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8046133,"sourceId":65626,"sourceType":"competition"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
