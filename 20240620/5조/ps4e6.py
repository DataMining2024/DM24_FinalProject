# -*- coding: utf-8 -*-
"""PS4E6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WweZ_0LTdDV-2RKgd7IkhP0iEQZgMgG9
"""

!pip install catboost

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestClassifier
from catboost import CatBoostClassifier
from IPython.display import HTML, display
from sklearn.ensemble import RandomForestClassifier
from catboost import CatBoostClassifier
from xgboost import XGBClassifier  #
from lightgbm import LGBMClassifier
from sklearn.metrics import classification_report
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings('ignore')
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

!pip install optuna

from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import (
    RandomForestClassifier,
    GradientBoostingClassifier,
    HistGradientBoostingClassifier
)
from optuna.samplers import TPESampler
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import warnings
import optuna
import pickle
import shutil
import os

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil



df_train = pd.read_csv('./train.csv')
df_test  = pd.read_csv('./test.csv')
submission = pd.read_csv('./sample_submission.csv')
# #droping id
df_train.drop(columns=['id'], inplace=True)
df_test.drop(columns=['id'], inplace=True)

df_train

df_test

null_counts = df_train.isnull().sum()

# 결측치가 있는 열 찾기
columns_with_null = null_counts[null_counts > 0].index.tolist()

# 결측치가 있는 열과 해당 열의 결측치 개수 출력
for column in columns_with_null:
    print(f"Column '{column}': {null_counts[column]} null values")

# 중복된 행 확인
duplicate_rows = df_train.duplicated()

# 중복된 행의 개수
duplicate_count = duplicate_rows.sum()

if duplicate_count == 0:
    print("No duplicate rows in the dataset.")
else:
    print(f"There are {duplicate_count} duplicate rows in the dataset.")

summary = df_train.describe(include='all')  # 모든 열에 대한 통계 정보 포함
print(summary)

data = df_train['Target'].value_counts()
labels = data.index
sizes = data.values

fig, ax = plt.subplots(figsize=(10, 6), subplot_kw=dict(aspect="equal"))

wedges, texts, autotexts = ax.pie(sizes,  labels=labels,  autopct='%1.1f%%', startangle=140)

plt.setp(texts, size=12)
plt.setp(autotexts, size=12)

ax.legend(wedges, labels, title="Targets")

ax.axis('equal')

plt.show()

df_train['Target'].unique()

colors = ['#ff00e6', '#030303', '#f2d5a5']
sns.set(style="whitegrid")
fig = plt.figure(figsize=(15, 60))
i = 1
for label in df_train.drop('Target', axis=1).columns:
    plt.subplot(13, 3, i)
    for idx, target in enumerate(df_train['Target'].unique()):
        sns.histplot(df_train[df_train['Target'] == target][label], color=colors[idx], label=target, alpha=0.5)
    plt.title(label, fontsize=16, fontweight='bold')
    plt.xlabel(label, fontsize=12)
    plt.ylabel('Probability', fontsize=12)
    plt.legend(fontsize=10)
    plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    i += 1

plt.tight_layout()
plt.show()

from sklearn.preprocessing import LabelEncoder
df_train2 = df_train.copy()
# LabelEncoder를 사용하여 'Education' 열을 숫자형으로 변환
label_encoder = LabelEncoder()
df_train2['Target'] = label_encoder.fit_transform(df_train['Target'])

corr_matrix = df_train2.corr()

# 히트맵 그리기
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Heatmap', fontsize=16)
plt.show()

threshold = 0.65

# Target 열과의 상관 행렬 추출
target_corr = corr_matrix['Target'].drop('Target')  # 자기 자신과의 상관 관계 제외

# 임계값 이상인 상관 계수를 가진 열 추출
high_corr_cols = target_corr[abs(target_corr) > threshold].index.tolist()

print("Columns with correlation coefficient higher than", threshold, "with Target:")
print(high_corr_cols)

def model_test(train_df, test_df, threshold, n_estimators_size, min_samples_split_size) :

    df_train2 = train_df.copy()
    df_train2['Target'] = label_encoder.fit_transform(df_train['Target'])
    df_test = test_df.copy()
    corr = df_train2.corr()
    drop_cols = []

    for label in df_train2.drop('Target', axis=1).columns:
        correlation = corr[label].abs()[-1]
        if correlation < threshold:
            drop_cols.append(label)

    df_train2 = df_train2.drop(drop_cols, axis=1)
    df_test = df_test.drop(drop_cols, axis=1)

    # models
    model1 = RandomForestClassifier(n_estimators=n_estimators_size, min_samples_split=min_samples_split_size)
    model2 = CatBoostClassifier(verbose=False, iterations=10000, use_best_model=True)
    model3 = XGBClassifier()
    model4 = LGBMClassifier()

    models = [model1, model2, model3, model4]

    MODELS = []
    accuracy_score_list = []

    X_train, X_val, y_train, y_val = train_test_split(df_train2.drop('Target', axis=1), df_train2['Target'], test_size=0.2, random_state=42)

    for model in models:
        try:
            # Train the model
            model.fit(X_train, y_train, eval_set=(X_val, y_val))
        except:
            model.fit(X_train, y_train)

        y_pred = model.predict(X_val)
        print(f'Model Name: {model.__class__.__name__}, score : ', accuracy_score(y_val, y_pred),'\n')
        #print(accuracy_score(y_val, y_pred))
        accuracy_score_list.append(accuracy_score(y_val, y_pred))
        max_index = accuracy_score_list.index(max(accuracy_score_list))
        MODELS.append(model)

    final_model = MODELS[max_index]

    y_pred = final_model.predict(df_test)

    y_pred_0 = MODELS[0].predict(df_test)
    y_pred_1 = MODELS[1].predict(df_test)
    y_pred_2 = MODELS[2].predict(df_test)
    y_pred_3 = MODELS[3].predict(df_test)

    preds = pd.DataFrame({
        'Model_0': y_pred_0,
        'Model_1': np.ravel(y_pred_1),
        'Model_2': y_pred_2,
        'Model_3': y_pred_3
    })
    blend_preds = preds.mode(axis=1)
    blend_preds = blend_preds.iloc[:, 0].astype(int)
    blend_preds = label_encoder.inverse_transform(blend_preds)

    ensemble_submission = pd.DataFrame({
        'id': submission['id'],
        'Target': blend_preds
    })

    # CSV 파일로 저장
    ensemble_submission.to_csv('ensemble_blend_submission_{}_{}_{}.csv'.format(threshold, n_estimators_size, min_samples_split_size), index=False)

    if y_pred.ndim == 2:
        y_pred = y_pred.reshape(-1)
    y_pred = pd.Series(y_pred)

    y_pred = label_encoder.inverse_transform(y_pred)

    one_submission = pd.DataFrame({
        'id': submission['id'],
        'Target': y_pred
    })
    one_submission.to_csv('submission_{}_{}_{}.csv'.format(threshold, n_estimators_size, min_samples_split_size), index=False)

"""
thresholds = [0.05, 0.07, 0.1, 0.12]
n_estimators_sizes = [150, 200, 250]
min_samples_split_sizes = [2, 3, 4]

for threshold in thresholds:
    for n_estimators_size in n_estimators_sizes:
        for min_samples_split_size in min_samples_split_sizes:
            model_test(df_train, df_test, threshold, n_estimators_size, min_samples_split_size)
"""

import multiprocessing

thresholds = [0.05, 0.07, 0.1, 0.12]
n_estimators_sizes = [150, 200, 250]
min_samples_split_sizes = [2, 3, 4]

def process_model(threshold, n_estimators_size, min_samples_split_size):
    model_test(df_train, df_test, threshold, n_estimators_size, min_samples_split_size)

if __name__ == "__main__":
    processes = []
    for threshold in thresholds:
        for n_estimators_size in n_estimators_sizes:
            for min_samples_split_size in min_samples_split_sizes:
                process = multiprocessing.Process(target=process_model, args=(threshold, n_estimators_size, min_samples_split_size))
                process.start()
                processes.append(process)

    for process in processes:
        process.join()

class Trainer:
    def __init__(self, model, n_folds=5):
        self.model = model
        self.n_folds = n_folds

    def fit(self, X, y):
        print(f'Training {self.model.__class__.__name__}')

        rskf = StratifiedKFold(n_splits=self.n_folds, random_state=27, shuffle=True)

        accs = []
        aucs = []
        oof_pred_probs = np.zeros((X.shape[0], len(np.unique(y))))
        oof_preds = np.zeros(X.shape[0])
        for fold_idx, (train_idx, val_idx) in enumerate(rskf.split(X, y)):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]


            y_pred_probs = self.model.predict_proba(X_val)
            y_pred = np.argmax(y_pred_probs, axis=1)

            oof_pred_probs[val_idx] = y_pred_probs
            oof_preds[val_idx] = y_pred

            acc = accuracy_score(y_val, y_pred)
            auc = roc_auc_score(y_val, y_pred_probs, multi_class='ovo')
            accs.append(acc)
            aucs.append(auc)

            print(f'--- Fold {fold_idx + 1} - Accuracy: {acc:.6f}, AUC: {auc:.6f}')

        print(f'\n------ Accuracy: {np.mean(accs):.6f} ± {np.std(accs):.6f}')
        print(f'------ AUC:      {np.mean(aucs):.6f} ± {np.std(aucs):.6f}\n\n')

        return self.model, oof_pred_probs, oof_preds, accs, aucs

X_train.isna().sum()

df_train2

df_test2

df_train = pd.read_csv('/content/train.csv')
df_test  = pd.read_csv('/content/test.csv')
submission = pd.read_csv('/content/sample_submission.csv')
# #droping id
df_train.drop(columns=['id'], inplace=True)
df_test.drop(columns=['id'], inplace=True)

train_df = df_train
test_df = df_test
threshold = 0.05
n_estimators_size = 200
min_samples_split_size = 4

accuracies = {}
aucs = {}

test_pred_probs = {}
oof_pred_probs = {}
oof_preds = {}

label_encoder = LabelEncoder()




df_train2 = train_df.copy()
df_test2 = test_df.copy()
df_train2['Target'] = label_encoder.fit_transform(df_train['Target'])
df_test = test_df.copy()
'''
corr = df_train2.corr()
drop_cols = []

for label in df_train2.drop('Target', axis=1).columns:
        correlation = corr[label].abs()[-1]
        if correlation < threshold:
            drop_cols.append(label)

df_train2 = df_train2.drop(drop_cols, axis=1)
df_test = df_test2.drop(drop_cols, axis=1)
'''

# models
model1 = RandomForestClassifier(n_estimators=n_estimators_size, min_samples_split=min_samples_split_size)
model2 = CatBoostClassifier(verbose=False, iterations=10000, use_best_model=True)
model3 = XGBClassifier()
model4 = LGBMClassifier()

models = [model1, model2, model3, model4]

MODELS = []
accuracy_score_list = []

X_train, X_val, y_train, y_val = train_test_split(df_train2.drop('Target', axis=1), df_train2['Target'], test_size=0.3, random_state=42)

X = df_train2.drop('Target', axis=1)
y = df_train2['Target']





for model in models:

    rskf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)

    for fold_idx, (train_idx, val_idx) in enumerate(rskf.split(X, y)):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            try :
                model.fit(X_train, y_train)
            except :
                model.fit(X_train, y_train, eval_set=(X_val, y_val))
    '''
    try:
            # Train the model
        model.fit(X_train, y_train, eval_set=(X_val, y_val))
    except:
        model.fit(X_train, y_train)
'''
    y_pred = model.predict(X_val)
    print(f'Model Name: {model.__class__.__name__}, score : ', accuracy_score(y_val, y_pred),'\n')
    #print(accuracy_score(y_val, y_pred))
    accuracy_score_list.append(accuracy_score(y_val, y_pred))
    max_index = accuracy_score_list.index(max(accuracy_score_list))
    MODELS.append(model)

final_model = MODELS[max_index]

y_pred = final_model.predict(df_test)

y_pred_0 = MODELS[0].predict(df_test)
y_pred_1 = MODELS[1].predict(df_test)
y_pred_2 = MODELS[2].predict(df_test)
y_pred_3 = MODELS[3].predict(df_test)



xgb_model = MODELS[2]
xgb_model, xgb_oof_pred_probs, xgb_oof_preds, xgb_accs, xgb_aucs = Trainer(xgb_model).fit(X, y)
oof_pred_probs['XGB'] = xgb_oof_pred_probs
oof_preds['XGB'] = xgb_oof_preds
test_pred_probs['XGB'] = xgb_model.predict_proba(df_test)
accuracies['XGB'] = xgb_accs
aucs['XGB'] = xgb_aucs


cb_model = MODELS[1]
cb_model, cb_oof_pred_probs, cb_oof_preds, cb_accs, cb_aucs = Trainer(cb_model).fit(X, y)
oof_pred_probs['CB'] = cb_oof_pred_probs
oof_preds['CB'] = cb_oof_preds
test_pred_probs['CB'] = cb_model.predict_proba(df_test)
accuracies['CB'] = cb_accs
aucs['CB'] = cb_aucs


lgbm_model = MODELS[3]
lgbm_model, lgbm_oof_pred_probs, lgbm_oof_preds, lgbm_accs, lgbm_aucs = Trainer(lgbm_model).fit(X, y)
oof_pred_probs['LGBM'] = lgbm_oof_pred_probs
oof_preds['LGBM'] = lgbm_oof_preds
test_pred_probs['LGBM'] = lgbm_model.predict_proba(df_test)
accuracies['LGBM'] = lgbm_accs
aucs['LGBM'] = lgbm_aucs


rf_model = MODELS[0]
rf_model, rf_oof_pred_probs, rf_oof_preds, rf_accs, rf_aucs = Trainer(rf_model).fit(X, y)
oof_pred_probs['RF'] = rf_oof_pred_probs
oof_preds['RF'] = rf_oof_preds
test_pred_probs['RF'] = rf_model.predict_proba(df_test)
accuracies['RF'] = rf_accs
aucs['RF'] = rf_aucs








preds = pd.DataFrame({
        'Model_0': y_pred_0,
        'Model_1': np.ravel(y_pred_1),
        'Model_2': y_pred_2,
        'Model_3': y_pred_3
    })

blend_preds = preds.mode(axis=1)
blend_preds = blend_preds.iloc[:, 0].astype(int)
blend_preds = label_encoder.inverse_transform(blend_preds)

ensemble_submission = pd.DataFrame({
        'id': submission['id'],
        'Target': blend_preds
    })

    # CSV 파일로 저장
ensemble_submission.to_csv('ensemble_blend_submission_{}_{}_{}.csv'.format(threshold, n_estimators_size, min_samples_split_size), index=False)

if y_pred.ndim == 2:
        y_pred = y_pred.reshape(-1)
y_pred = pd.Series(y_pred)

y_pred = label_encoder.inverse_transform(y_pred)

one_submission = pd.DataFrame({
        'id': submission['id'],
        'Target': y_pred
    })
one_submission.to_csv('submission_{}_{}_{}.csv'.format(threshold, n_estimators_size, min_samples_split_size), index=False)

def objective(trial):
    xgb_weight = trial.suggest_float('xgb_weight', 0.0, 1.0)
    cb_weight = trial.suggest_float('cb_weight', 0.0, 1.0)
    lgbm_weight = trial.suggest_float('lgbm_weight', 0.0, 1.0)
    rf_weight = trial.suggest_float('rf_weight', 0.0, 1.0)

    weights = [
        xgb_weight,
        cb_weight,
        lgbm_weight,
        rf_weight
    ]
    weights /= np.sum(weights)

    pred_probs = np.zeros((X.shape[0], len(np.unique(y))))
    for model, weight in zip(oof_pred_probs.keys(), weights):
        pred_probs += oof_pred_probs[model] * weight

    preds = np.argmax(pred_probs, axis=1)

    return accuracy_score(y, preds)


sampler = TPESampler(seed=27)
study = optuna.create_study(direction='maximize', sampler=sampler)
study.optimize(objective, n_trials=1000)

best_weights = study.best_params
best_weights = [best_weights[f'{model}_weight'] for model in ['xgb', 'cb', 'lgbm', 'rf']]
best_weights /= np.sum(best_weights)

print(f'Ensemble weights: {best_weights}')

ensemble_oof_pred_probs = np.zeros((X.shape[0], len(np.unique(y))))
for model, weight in zip(oof_pred_probs.keys(), best_weights):
    ensemble_oof_pred_probs += oof_pred_probs[model] * weight

ensemble_oof_preds = np.argmax(ensemble_oof_pred_probs, axis=1)
ensemble_acc = accuracy_score(y, ensemble_oof_preds)
ensemble_auc = roc_auc_score(y, ensemble_oof_pred_probs, multi_class='ovo')

oof_pred_probs['Ensemble'] = ensemble_oof_pred_probs
oof_preds['Ensemble'] = ensemble_oof_preds
accuracies['Ensemble'] = [ensemble_acc] * 5
aucs['Ensemble'] = [ensemble_auc] * 5

accuracies = pd.DataFrame(accuracies)
aucs = pd.DataFrame(aucs)

final_pred_probs = np.zeros((df_test.shape[0], len(np.unique(y))))
for model, weight in zip(test_pred_probs.keys(), best_weights):
    final_pred_probs += test_pred_probs[model] * weight

final_preds = np.argmax(final_pred_probs, axis=1)
final_preds = label_encoder.inverse_transform(final_preds)

submission = pd.DataFrame({'id': submission['id'], 'Target': final_preds})
submission.to_csv(f"ensemble_acc-{np.mean(accuracies['Ensemble']):.6f}_auc-{np.mean(aucs['Ensemble']):.6f}.csv", index=False)

np.mean(accuracies['Ensemble'])

submission['id']

os.mkdir('oof_pred_probs')
os.mkdir('test_pred_probs')

for model, pred_probs in oof_pred_probs.items():
    with open(f'oof_pred_probs/{model.lower()}_oof_pred_probs.pkl', 'wb') as f:
        pickle.dump(pred_probs, f)

for model, pred_probs in test_pred_probs.items():
    with open(f'test_pred_probs/{model.lower()}_test_pred_probs.pkl', 'wb') as f:
        pickle.dump(pred_probs, f)

y_pred = final_model.predict(df_test)

y_pred_0 = MODELS[0].predict(df_test)
y_pred_1 = MODELS[1].predict(df_test)
y_pred_2 = MODELS[2].predict(df_test)
y_pred_3 = MODELS[3].predict(df_test)

preds = pd.DataFrame({
    'Model_0': y_pred_0,
    'Model_1': np.ravel(y_pred_1),
    'Model_2': y_pred_2,
    'Model_3': y_pred_3
})
blend_preds = preds.mode(axis=1)
blend_preds = blend_preds.iloc[:, 0].astype(int)
blend_preds = label_encoder.inverse_transform(blend_preds)

submission = pd.DataFrame({
    'id': submission['id'],
    'Target': blend_preds
})

# CSV 파일로 저장
submission.to_csv('ensemble_blend_submission.csv', index=False)

if y_pred.ndim == 2:
    y_pred = y_pred.reshape(-1)
y_pred = pd.Series(y_pred)

y_pred = label_encoder.inverse_transform(y_pred)
y_pred

submission['Target'] = y_pred
submission.head()

submission.to_csv('submission.csv', index=False)

preds = pd.DataFrame({
    'Model_0': y_pred_0,
    'Model_1': np.ravel(y_pred_1),
    'Model_2': y_pred_2,
    'Model_3': y_pred_3
})
# 각 행별로 모드(가장 많이 나타나는 값)를 계산
blend_preds = preds.mode(axis=1)
blend_preds

blend_preds = blend_preds.iloc[:, 0].astype(int)
blend_preds

blend_preds = label_encoder.inverse_transform(blend_preds)

submission = pd.DataFrame({
    'id': submission['id'],
    'Target': blend_preds
})

# CSV 파일로 저장
submission.to_csv('ensemble_blend_submission.csv', index=False)

y_pred

if y_pred.ndim == 2:
    y_pred = y_pred.reshape(-1)
y_pred = pd.Series(y_pred)

y_pred = label_encoder.inverse_transform(y_pred)
y_pred

submission['Target'] = y_pred
submission.head()

submission.to_csv('submission.csv', index=False)