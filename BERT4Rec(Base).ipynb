{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QgpnZLRbGDO1","outputId":"154294b8-2bfd-406e-a001-11ef2e9dd7e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: python-box in /usr/local/lib/python3.10/dist-packages (7.1.1)\n"]}],"source":["!pip install python-box"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i5bRFjg4FRmB","outputId":"6ba1e418-dd9e-4f95-c0ed-677a06cb531f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EHDoRF4_BlnC"},"outputs":[],"source":["import math\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from collections import defaultdict\n","import os\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","from box import Box\n","\n","import warnings\n","\n","warnings.filterwarnings(action='ignore')\n","torch.set_printoptions(sci_mode=True)"]},{"cell_type":"markdown","metadata":{"id":"dKxmZatGf3OR"},"source":["1. 학습설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FRJ84ZD-GIyD"},"outputs":[],"source":["config = {\n","    #'data_path' : \"/content/drive/MyDrive/RecsysTutorial/Data/MovieLens\" , # 데이터 경로\n","    'data_path' : \"/content/drive/MyDrive/datamining1\" , # 데이터 경로\n","\n","    'max_len' : 50,\n","    'hidden_units' : 50, # Embedding size\n","    'num_heads' : 1, # Multi-head layer 의 수 (병렬 처리)\n","    'num_layers': 2, # block의 개수 (encoder layer의 개수)\n","    'dropout_rate' : 0.5, # dropout 비율\n","    'lr' : 0.001,\n","    'batch_size' : 128,\n","    'num_epochs' : 50,\n","    'num_workers' : 2,\n","    'mask_prob' : 0.15, # for cloze task\n","}\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","config = Box(config)"]},{"cell_type":"markdown","metadata":{"id":"uml8yYfSI1uK"},"source":["2. 데이터 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aqdj3DRa4Z6h"},"outputs":[],"source":["class MakeSequenceDataSet():\n","    \"\"\"\n","    SequenceData 생성\n","    \"\"\"\n","    def __init__(self, config):\n","        self.config = config\n","        self.df = pd.read_csv(os.path.join(self.config.data_path, 'ratings.csv'))\n","\n","        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('movie_id')\n","        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('user_id')\n","        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n","\n","        self.df['item_idx'] = self.df['movie_id'].apply(lambda x : self.item_encoder[x] + 1)\n","        self.df['user_idx'] = self.df['user_id'].apply(lambda x : self.user_encoder[x])\n","        self.df = self.df.sort_values(['user_idx', 'timestamp']) # 시간에 따라 정렬\n","        self.user_train, self.user_valid = self.generate_sequence_data()\n","\n","    def generate_encoder_decoder(self, col : str) -> dict:\n","        \"\"\"\n","        encoder, decoder 생성\n","\n","        Args:\n","            col (str): 생성할 columns 명\n","        Returns:\n","            dict: 생성된 user encoder, decoder\n","        \"\"\"\n","\n","        encoder = {}\n","        decoder = {}\n","        ids = self.df[col].unique()\n","\n","        for idx, _id in enumerate(ids):\n","            encoder[_id] = idx\n","            decoder[idx] = _id\n","\n","        return encoder, decoder\n","\n","    def generate_sequence_data(self) -> dict:\n","        \"\"\"\n","        sequence_data 생성\n","\n","        Returns:\n","            dict: train user sequence / valid user sequence\n","        \"\"\"\n","        users = defaultdict(list)\n","        user_train = {}\n","        user_valid = {}\n","        group_df = self.df.groupby('user_idx')\n","        for user, item in group_df:\n","            users[user].extend(item['item_idx'].tolist())\n","\n","        for user in users:\n","            user_train[user] = users[user][:-1]\n","            user_valid[user] = [users[user][-1]] # 마지막 아이템을 예측\n","\n","        return user_train, user_valid\n","\n","    def get_train_valid_data(self):\n","        return self.user_train, self.user_valid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kpi0ZbWYGPI3"},"outputs":[],"source":["class BERTRecDataSet(Dataset):\n","    def __init__(self, user_train, max_len, num_user, num_item, mask_prob):\n","        self.user_train = user_train\n","        self.max_len = max_len\n","        self.num_user = num_user\n","        self.num_item = num_item\n","        self.mask_prob = mask_prob\n","        self._all_items = set([i for i in range(1, self.num_item + 1)])\n","\n","    def __len__(self):\n","        # 총 user의 수 = 학습에 사용할 sequence의 수\n","        return self.num_user\n","\n","    def __getitem__(self, user):\n","\n","        user_seq = self.user_train[user]\n","        tokens = []\n","        labels = []\n","        for s in user_seq[-self.max_len:]:\n","            prob = np.random.random()\n","            if prob < self.mask_prob:\n","                prob /= self.mask_prob\n","                if prob < 0.8:\n","                    # masking\n","                    tokens.append(self.num_item + 1)  # mask_index: num_item + 1, 0: pad, 1~num_item: item index\n","                elif prob < 0.9:\n","                    # noise\n","                    tokens.extend(self.random_neg_sampling(rated_item = user_seq, num_item_sample = 1))  # item random sampling\n","                else:\n","                    tokens.append(s)\n","                labels.append(s) # 학습에 사용 O\n","            else:\n","                tokens.append(s)\n","                labels.append(0) # 학습에 사용 X\n","\n","        mask_len = self.max_len - len(tokens)\n","        tokens = [0] * mask_len + tokens\n","        labels = [0] * mask_len + labels\n","\n","        return torch.LongTensor(tokens), torch.LongTensor(labels)\n","\n","    def random_neg_sampling(self, rated_item : list, num_item_sample : int):\n","        nge_samples = random.sample(list(self._all_items - set(rated_item)), num_item_sample)\n","        return nge_samples"]},{"cell_type":"markdown","metadata":{"id":"DYsR4jdWgJdA"},"source":["3. 모델"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tctK2vnsGQQi"},"outputs":[],"source":["class ScaledDotProductAttention(nn.Module):\n","    def __init__(self, hidden_units, dropout_rate):\n","        super(ScaledDotProductAttention, self).__init__()\n","        self.hidden_units = hidden_units\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, Q, K, V, mask):\n","        \"\"\"\n","        Q, K, V : (batch_size, num_heads, max_len, hidden_units)\n","        mask : (batch_size, 1, max_len, max_len)\n","        \"\"\"\n","        attn_score = torch.matmul(Q, K.transpose(2, 3)) / math.sqrt(self.hidden_units) # (batch_size, num_heads, max_len, max_len)\n","        attn_score = attn_score.masked_fill(mask == 0, -1e9)  # 유사도가 0인 지점은 -infinity로 보내 softmax 결과가 0이 되도록 함\n","        attn_dist = self.dropout(F.softmax(attn_score, dim=-1))  # attention distribution\n","        output = torch.matmul(attn_dist, V)  # (batch_size, num_heads, max_len, hidden_units) / # dim of output : batchSize x num_head x seqLen x hidden_units\n","        return output, attn_dist\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, num_heads, hidden_units, dropout_rate):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads # head의 수\n","        self.hidden_units = hidden_units\n","\n","        # query, key, value, output 생성을 위해 Linear 모델 생성\n","        self.W_Q = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n","        self.W_K = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n","        self.W_V = nn.Linear(hidden_units, hidden_units * num_heads, bias=False)\n","        self.W_O = nn.Linear(hidden_units * num_heads, hidden_units, bias=False)\n","\n","        self.attention = ScaledDotProductAttention(hidden_units, dropout_rate)\n","        self.dropout = nn.Dropout(dropout_rate) # dropout rate\n","        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n","\n","    def forward(self, enc, mask):\n","        \"\"\"\n","        enc : (batch_size, max_len, hidden_units)\n","        mask : (batch_size, 1, max_len, max_len)\n","\n","        \"\"\"\n","        residual = enc # residual connection을 위해 residual 부분을 저장\n","        batch_size, seqlen = enc.size(0), enc.size(1)\n","\n","        # Query, Key, Value를 (num_head)개의 Head로 나누어 각기 다른 Linear projection을 통과시킴\n","        Q = self.W_Q(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) # (batch_size, max_len, num_heads, hidden_units)\n","        K = self.W_K(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) # (batch_size, max_len, num_heads, hidden_units)\n","        V = self.W_V(enc).view(batch_size, seqlen, self.num_heads, self.hidden_units) # (batch_size, max_len, num_heads, hidden_units)\n","\n","        # Head별로 각기 다른 attention이 가능하도록 Transpose 후 각각 attention에 통과시킴\n","        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2) # (batch_size, num_heads, max_len, hidden_units)\n","        output, attn_dist = self.attention(Q, K, V, mask) # output : (batch_size, num_heads, max_len, hidden_units) / attn_dist : (batch_size, num_heads, max_len, max_len)\n","\n","        # 다시 Transpose한 후 모든 head들의 attention 결과를 합칩니다.\n","        output = output.transpose(1, 2).contiguous() # (batch_size, max_len, num_heads, hidden_units) / contiguous() : 가변적 메모리 할당\n","        output = output.view(batch_size, seqlen, -1) # (batch_size, max_len, hidden_units * num_heads)\n","\n","        # Linear Projection, Dropout, Residual sum, and Layer Normalization\n","        output = self.layerNorm(self.dropout(self.W_O(output)) + residual) # (batch_size, max_len, hidden_units)\n","        return output, attn_dist\n","\n","\n","class PositionwiseFeedForward(nn.Module):\n","    def __init__(self, hidden_units, dropout_rate):\n","        super(PositionwiseFeedForward, self).__init__()\n","\n","        self.W_1 = nn.Linear(hidden_units, hidden_units)\n","        self.W_2 = nn.Linear(hidden_units, hidden_units)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.layerNorm = nn.LayerNorm(hidden_units, 1e-6) # layer normalization\n","\n","    def forward(self, x):\n","        residual = x\n","        output = self.W_2(F.relu(self.dropout(self.W_1(x))))\n","        output = self.layerNorm(self.dropout(output) + residual)\n","        return output\n","\n","\n","class BERT4RecBlock(nn.Module):\n","    def __init__(self, num_heads, hidden_units, dropout_rate):\n","        super(BERT4RecBlock, self).__init__()\n","        self.attention = MultiHeadAttention(num_heads, hidden_units, dropout_rate)\n","        self.pointwise_feedforward = PositionwiseFeedForward(hidden_units, dropout_rate)\n","\n","    def forward(self, input_enc, mask):\n","        output_enc, attn_dist = self.attention(input_enc, mask)\n","        output_enc = self.pointwise_feedforward(output_enc)\n","        return output_enc, attn_dist\n","\n","\n","class BERT4Rec(nn.Module):\n","    def __init__(self, num_user, num_item, hidden_units, num_heads, num_layers, max_len, dropout_rate, device):\n","        super(BERT4Rec, self).__init__()\n","\n","        self.num_user = num_user\n","        self.num_item = num_item\n","        self.hidden_units = hidden_units\n","        self.num_heads = num_heads\n","        self.num_layers = num_layers\n","        self.device = device\n","\n","        self.item_emb = nn.Embedding(num_item + 2, hidden_units, padding_idx=0) # padding : 0 / item : 1 ~ num_item + 1 /  mask : num_item + 2\n","        self.pos_emb = nn.Embedding(max_len, hidden_units) # learnable positional encoding\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.emb_layernorm = nn.LayerNorm(hidden_units, eps=1e-6)\n","\n","        self.blocks = nn.ModuleList([BERT4RecBlock(num_heads, hidden_units, dropout_rate) for _ in range(num_layers)])\n","        self.out = nn.Linear(hidden_units, num_item + 1)\n","\n","    def forward(self, log_seqs):\n","        \"\"\"\n","        log_seqs : (batch_size, max_len)\n","\n","        ex)\n","        log_seqs = [\n","                [1, 2, 3, 4, 5],\n","                [0, 0, 0, 1, 2],\n","                [0, 0, 1, 2, 3]\n","        ]\n","\n","        \"\"\"\n","        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.device)) # (batch_size, max_len, hidden_units)\n","        positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1]) # (batch_size, max_len)\n","        seqs += self.pos_emb(torch.LongTensor(positions).to(self.device)) # (batch_size, max_len, hidden_units)\n","        seqs = self.emb_layernorm(self.dropout(seqs)) # LayerNorm\n","\n","        mask_pad = torch.BoolTensor(log_seqs > 0).unsqueeze(1).repeat(1, log_seqs.shape[1], 1).unsqueeze(1).to(self.device) # mask for zero pad / (batch_size, 1, max_len, max_len)\n","        for block in self.blocks:\n","            seqs, attn_dist = block(seqs, mask_pad)\n","        out = self.out(seqs) # (batch_size, max_len, num_item + 1)\n","        return out\n"]},{"cell_type":"markdown","metadata":{"id":"CViPzQbRhgmC"},"source":["아래에 PPA 전처리 추가"]},{"cell_type":"markdown","metadata":{"id":"CVk1pAXygPM8"},"source":["4. 학습 함수"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRPLZiLSGSP8"},"outputs":[],"source":["def train(model, criterion, optimizer, data_loader):\n","    model.train()\n","    loss_val = 0\n","    for seq, labels in data_loader:\n","        logits = model(seq)\n","\n","        logits = logits.view(-1, logits.size(-1))\n","        labels = labels.view(-1).to(device)\n","\n","        optimizer.zero_grad()\n","        loss = criterion(logits, labels)\n","\n","        loss_val += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    loss_val /= len(data_loader)\n","\n","    return loss_val\n","\n","def evaluate(model, user_train, user_valid, max_len, bert4rec_dataset, make_sequence_dataset):\n","    model.eval()\n","\n","    NDCG = 0.0 # NDCG@10\n","    HIT = 0.0 # HIT@10\n","\n","    num_item_sample = 100\n","\n","    users = [user for user in range(make_sequence_dataset.num_user)]\n","\n","    for user in users:\n","        seq = (user_train[user] + [make_sequence_dataset.num_item + 1])[-max_len:]\n","        rated = user_train[user] + user_valid[user]\n","        items = user_valid[user] + bert4rec_dataset.random_neg_sampling(rated_item = rated, num_item_sample = num_item_sample)\n","\n","        with torch.no_grad():\n","            predictions = -model(np.array([seq]))\n","            predictions = predictions[0][-1][items] # sampling\n","            rank = predictions.argsort().argsort()[0].item()\n","\n","        if rank < 10: #Top10\n","            NDCG += 1 / np.log2(rank + 2)\n","            HIT += 1\n","\n","    NDCG /= len(users)\n","    HIT /= len(users)\n","\n","    return NDCG, HIT"]},{"cell_type":"markdown","metadata":{"id":"jMKBnRJjgSzg"},"source":["5. 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_TU0VWGGTmn"},"outputs":[],"source":["make_sequence_dataset = MakeSequenceDataSet(config = config)\n","user_train, user_valid = make_sequence_dataset.get_train_valid_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3UUPRPiGVOi"},"outputs":[],"source":["bert4rec_dataset = BERTRecDataSet(\n","    user_train = user_train,\n","    max_len = config.max_len,\n","    num_user = make_sequence_dataset.num_user,\n","    num_item = make_sequence_dataset.num_item,\n","    mask_prob = config.mask_prob,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3nKrmtYoGZyW"},"outputs":[],"source":["data_loader = DataLoader(\n","    bert4rec_dataset,\n","    batch_size = config.batch_size,\n","    shuffle = True,\n","    pin_memory = True,\n","    num_workers = config.num_workers,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"13NXJYNtGZwU"},"outputs":[],"source":["model = BERT4Rec(\n","    num_user = make_sequence_dataset.num_user,\n","    num_item = make_sequence_dataset.num_item,\n","    hidden_units = config.hidden_units,\n","    num_heads = config.num_heads,\n","    num_layers = config.num_layers,\n","    max_len = config.max_len,\n","    dropout_rate = config.dropout_rate,\n","    device = device,\n","    ).to(device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0) # label이 0인 경우 무시\n","optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p_lScofuGZuV","outputId":"2c889cf9-f9b1-4664-adc3-c19bd1a378f1"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch:   1| Train loss: 8.06117| NDCG@10: 0.21847| HIT@10: 0.41556: 100%|██████████| 1/1 [00:20<00:00, 20.42s/it]\n","Epoch:   2| Train loss: 7.56649| NDCG@10: 0.25713| HIT@10: 0.46738: 100%|██████████| 1/1 [00:16<00:00, 16.15s/it]\n","Epoch:   3| Train loss: 7.49677| NDCG@10: 0.26213| HIT@10: 0.46921: 100%|██████████| 1/1 [00:14<00:00, 14.59s/it]\n","Epoch:   4| Train loss: 7.47636| NDCG@10: 0.25970| HIT@10: 0.47086: 100%|██████████| 1/1 [00:15<00:00, 15.18s/it]\n","Epoch:   5| Train loss: 7.47328| NDCG@10: 0.26319| HIT@10: 0.47235: 100%|██████████| 1/1 [00:15<00:00, 15.91s/it]\n","Epoch:   6| Train loss: 7.46227| NDCG@10: 0.26240| HIT@10: 0.47500: 100%|██████████| 1/1 [00:14<00:00, 14.73s/it]\n","Epoch:   7| Train loss: 7.47367| NDCG@10: 0.26372| HIT@10: 0.47434: 100%|██████████| 1/1 [00:14<00:00, 14.89s/it]\n","Epoch:   8| Train loss: 7.46321| NDCG@10: 0.26177| HIT@10: 0.46987: 100%|██████████| 1/1 [00:14<00:00, 14.68s/it]\n","Epoch:   9| Train loss: 7.45659| NDCG@10: 0.26495| HIT@10: 0.47550: 100%|██████████| 1/1 [00:14<00:00, 14.77s/it]\n","Epoch:  10| Train loss: 7.45063| NDCG@10: 0.26210| HIT@10: 0.47550: 100%|██████████| 1/1 [00:15<00:00, 15.30s/it]\n","Epoch:  11| Train loss: 7.45298| NDCG@10: 0.26035| HIT@10: 0.47103: 100%|██████████| 1/1 [00:14<00:00, 14.71s/it]\n","Epoch:  12| Train loss: 7.45373| NDCG@10: 0.26307| HIT@10: 0.47666: 100%|██████████| 1/1 [00:14<00:00, 14.61s/it]\n","Epoch:  13| Train loss: 7.45494| NDCG@10: 0.26461| HIT@10: 0.47666: 100%|██████████| 1/1 [00:15<00:00, 15.48s/it]\n","Epoch:  14| Train loss: 7.44181| NDCG@10: 0.26355| HIT@10: 0.47252: 100%|██████████| 1/1 [00:15<00:00, 15.48s/it]\n","Epoch:  15| Train loss: 7.44810| NDCG@10: 0.26587| HIT@10: 0.47599: 100%|██████████| 1/1 [00:14<00:00, 14.62s/it]\n","Epoch:  16| Train loss: 7.44788| NDCG@10: 0.26163| HIT@10: 0.46738: 100%|██████████| 1/1 [00:17<00:00, 17.58s/it]\n","Epoch:  17| Train loss: 7.45463| NDCG@10: 0.26420| HIT@10: 0.47252: 100%|██████████| 1/1 [00:14<00:00, 14.79s/it]\n","Epoch:  18| Train loss: 7.44257| NDCG@10: 0.25760| HIT@10: 0.46871: 100%|██████████| 1/1 [00:14<00:00, 14.64s/it]\n","Epoch:  19| Train loss: 7.43829| NDCG@10: 0.26227| HIT@10: 0.47285: 100%|██████████| 1/1 [00:14<00:00, 14.71s/it]\n","Epoch:  20| Train loss: 7.43365| NDCG@10: 0.26404| HIT@10: 0.47533: 100%|██████████| 1/1 [00:15<00:00, 15.46s/it]\n","Epoch:  21| Train loss: 7.42578| NDCG@10: 0.26141| HIT@10: 0.47318: 100%|██████████| 1/1 [00:15<00:00, 15.60s/it]\n","Epoch:  22| Train loss: 7.41749| NDCG@10: 0.25789| HIT@10: 0.46722: 100%|██████████| 1/1 [00:14<00:00, 14.79s/it]\n","Epoch:  23| Train loss: 7.40075| NDCG@10: 0.26490| HIT@10: 0.47682: 100%|██████████| 1/1 [00:14<00:00, 14.68s/it]\n","Epoch:  24| Train loss: 7.38966| NDCG@10: 0.26214| HIT@10: 0.47467: 100%|██████████| 1/1 [00:14<00:00, 14.95s/it]\n","Epoch:  25| Train loss: 7.37697| NDCG@10: 0.26303| HIT@10: 0.47848: 100%|██████████| 1/1 [00:14<00:00, 14.73s/it]\n","Epoch:  26| Train loss: 7.35812| NDCG@10: 0.26277| HIT@10: 0.47914: 100%|██████████| 1/1 [00:14<00:00, 14.63s/it]\n","Epoch:  27| Train loss: 7.34677| NDCG@10: 0.26733| HIT@10: 0.48394: 100%|██████████| 1/1 [00:14<00:00, 14.71s/it]\n","Epoch:  28| Train loss: 7.33383| NDCG@10: 0.26795| HIT@10: 0.48444: 100%|██████████| 1/1 [00:14<00:00, 14.51s/it]\n","Epoch:  29| Train loss: 7.31561| NDCG@10: 0.26500| HIT@10: 0.47980: 100%|██████████| 1/1 [00:15<00:00, 15.33s/it]\n","Epoch:  30| Train loss: 7.31148| NDCG@10: 0.27090| HIT@10: 0.48609: 100%|██████████| 1/1 [00:15<00:00, 15.65s/it]\n","Epoch:  31| Train loss: 7.30543| NDCG@10: 0.26541| HIT@10: 0.47616: 100%|██████████| 1/1 [00:14<00:00, 14.65s/it]\n","Epoch:  32| Train loss: 7.30407| NDCG@10: 0.26925| HIT@10: 0.49040: 100%|██████████| 1/1 [00:14<00:00, 14.75s/it]\n","Epoch:  33| Train loss: 7.28185| NDCG@10: 0.27045| HIT@10: 0.48493: 100%|██████████| 1/1 [00:14<00:00, 14.72s/it]\n","  0%|          | 0/1 [00:00<?, ?it/s]"]}],"source":["loss_list = []\n","ndcg_list = []\n","hit_list = []\n","for epoch in range(1, config.num_epochs + 1):\n","    tbar = tqdm(range(1))\n","    for _ in tbar:\n","        train_loss = train(\n","            model = model,\n","            criterion = criterion,\n","            optimizer = optimizer,\n","            data_loader = data_loader)\n","\n","        ndcg, hit = evaluate(\n","            model = model,\n","            user_train = user_train,\n","            user_valid = user_valid,\n","            max_len = config.max_len,\n","            bert4rec_dataset = bert4rec_dataset,\n","            make_sequence_dataset = make_sequence_dataset,\n","            )\n","\n","        loss_list.append(train_loss)\n","        ndcg_list.append(ndcg)\n","        hit_list.append(hit)\n","\n","        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZIa6bMmGZsf"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n","ax = ax.flatten()\n","epochs = [i for i in range(1, config.num_epochs + 1)]\n","\n","ax[0].plot(epochs, loss_list)\n","ax[0].set_title('Loss')\n","\n","ax[1].plot(epochs, ndcg_list)\n","ax[1].set_title('NDCG')\n","\n","ax[2].plot(epochs, hit_list)\n","ax[2].set_title('HIT')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}