{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":68699,"databundleVersionId":7659021,"sourceType":"competition"},{"sourceId":3972,"sourceType":"datasetVersion","datasetId":2363}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nimport optuna\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nfrom functools import partial\nfrom scipy.optimize import minimize","metadata":{"execution":{"iopub.status.busy":"2024-06-20T07:41:32.244712Z","iopub.execute_input":"2024-06-20T07:41:32.245141Z","iopub.status.idle":"2024-06-20T07:41:37.892508Z","shell.execute_reply.started":"2024-06-20T07:41:32.245108Z","shell.execute_reply":"2024-06-20T07:41:37.891143Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/playground-series-s4e3/'\n\ntrain_data = pd.read_csv(path + 'train.csv')\ntest_data = pd.read_csv(path + 'test.csv')\noriginal_data = pd.read_csv('/kaggle/input/faulty-steel-plates/' + 'faults.csv')\n\nTARGETS = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains','Dirtiness', 'Bumps', 'Other_Faults']\n\ntrain_data.drop(['id'],axis = 1,inplace = True)\ntest_data.drop(['id'],inplace = True,axis = 1)\n\n# 원본 데이터 합치기\ntrain_data = pd.concat([train_data,original_data],axis = 0)\ntrain_data.reset_index(drop=True, inplace=True)\n\nprint(train_data.shape)\nprint(test_data.shape)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-20T07:41:55.446761Z","iopub.execute_input":"2024-06-20T07:41:55.447413Z","iopub.status.idle":"2024-06-20T07:41:55.741192Z","shell.execute_reply.started":"2024-06-20T07:41:55.447377Z","shell.execute_reply":"2024-06-20T07:41:55.739905Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(21160, 34)\n(12814, 27)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = train_data[train_data[TARGETS].sum(axis=1) <= 1]\n# XGB 쓰려면 해야됨\ntrain_data['Outside_Global_Index'] = np.where(train_data['Outside_Global_Index']==0.7, 0.5, train_data['Outside_Global_Index'])\ntargets_bin = train_data[TARGETS]\ny_xgb = targets_bin","metadata":{"execution":{"iopub.status.busy":"2024-06-20T07:41:57.766575Z","iopub.execute_input":"2024-06-20T07:41:57.767100Z","iopub.status.idle":"2024-06-20T07:41:57.793658Z","shell.execute_reply.started":"2024-06-20T07:41:57.767061Z","shell.execute_reply":"2024-06-20T07:41:57.792429Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# 7개의 결함 중에서 어느것에 속하는 지 'Target'으로 표시하고 만약 어느것에도 해당하지 아니하면 \n# 0이 부여된다.\ntrain_data['Target'] = np.argmax(train_data[TARGETS].values, axis=1) + 1\ntrain_data.loc[train_data[TARGETS].sum(axis=1) == 0, 'Target'] = 0\ntrain_data.drop(TARGETS, inplace=True,axis =1)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T07:41:59.218796Z","iopub.execute_input":"2024-06-20T07:41:59.219250Z","iopub.status.idle":"2024-06-20T07:41:59.240513Z","shell.execute_reply.started":"2024-06-20T07:41:59.219218Z","shell.execute_reply":"2024-06-20T07:41:59.239282Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_data['Ratio_Length_Thickness'] = train_data['Length_of_Conveyer'] / train_data['Steel_Plate_Thickness']\ntrain_data['Average_Luminosity'] = train_data['Sum_of_Luminosity'] / train_data['Pixels_Areas']\ntrain_data['X_Range*Pixels_Areas'] = (train_data['X_Maximum'] - train_data['X_Minimum']) * train_data['Pixels_Areas']\n\ntest_data['Ratio_Length_Thickness'] = test_data['Length_of_Conveyer'] / test_data['Steel_Plate_Thickness']\ntest_data['Average_Luminosity'] = test_data['Sum_of_Luminosity'] / test_data['Pixels_Areas']\ntest_data['X_Range*Pixels_Areas'] = (test_data['X_Maximum'] - test_data['X_Minimum']) * train_data['Pixels_Areas']","metadata":{"execution":{"iopub.status.busy":"2024-06-20T07:41:59.963557Z","iopub.execute_input":"2024-06-20T07:41:59.964009Z","iopub.status.idle":"2024-06-20T07:41:59.988151Z","shell.execute_reply.started":"2024-06-20T07:41:59.963976Z","shell.execute_reply":"2024-06-20T07:41:59.986909Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# 클러스터링에 사용할 특성 선택\nfeatures = ['X_Minimum', 'Y_Minimum', 'Pixels_Areas', 'Sum_of_Luminosity', 'Steel_Plate_Thickness']\n# 클러스터링 모델 생성 및 학습\nkmeans = KMeans(n_clusters=4)\n\nkmeans.fit(train_data[features])\n# train 데이터에 클러스터링 결과 추가\ntrain_data['Cluster'] = kmeans.labels_\n# test 데이터에 클러스터링 결과 추가\ntest_data['Cluster'] = kmeans.predict(test_data[features])","metadata":{"execution":{"iopub.status.busy":"2024-06-20T07:42:00.738332Z","iopub.execute_input":"2024-06-20T07:42:00.738745Z","iopub.status.idle":"2024-06-20T07:42:06.315954Z","shell.execute_reply.started":"2024-06-20T07:42:00.738709Z","shell.execute_reply":"2024-06-20T07:42:06.314879Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# log scaling \nfor col in ['X_Perimeter', 'Pixels_Areas']:\n    train_data[col] = np.log1p(train_data[col])\n    test_data[col] = np.log1p(test_data[col])","metadata":{"execution":{"iopub.status.busy":"2024-06-20T07:42:13.136978Z","iopub.execute_input":"2024-06-20T07:42:13.137422Z","iopub.status.idle":"2024-06-20T07:42:13.149219Z","shell.execute_reply.started":"2024-06-20T07:42:13.137380Z","shell.execute_reply":"2024-06-20T07:42:13.148080Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"features_to_drop = ['Y_Minimum', 'Steel_Plate_Thickness', 'Sum_of_Luminosity', 'Edges_X_Index', 'SigmoidOfAreas', 'Luminosity_Index']\n\ntrain_data = train_data.drop(features_to_drop,axis = 1)\ntest_data = test_data.drop(features_to_drop,axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T07:42:13.922885Z","iopub.execute_input":"2024-06-20T07:42:13.923288Z","iopub.status.idle":"2024-06-20T07:42:13.936646Z","shell.execute_reply.started":"2024-06-20T07:42:13.923258Z","shell.execute_reply":"2024-06-20T07:42:13.935331Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X = train_data.drop(['Target'], axis=1)  \ny = train_data['Target']  \ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T07:42:14.618127Z","iopub.execute_input":"2024-06-20T07:42:14.618581Z","iopub.status.idle":"2024-06-20T07:42:14.629278Z","shell.execute_reply.started":"2024-06-20T07:42:14.618545Z","shell.execute_reply":"2024-06-20T07:42:14.627975Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### XGB","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    # Define hyperparameters to tune\n    param = {\n        'grow_policy': trial.suggest_categorical('grow_policy', [\"depthwise\", \"lossguide\"]),\n#         'multi_strategy': trial.suggest_categorical('multi_strategy', [\"one_output_per_tree\"]),\n#         'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n#         'gamma' : trial.suggest_float('gamma', 1e-5, 0.5, log=True),\n#         'subsample': trial.suggest_float('subsample', 0.3, 1.0),\n#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n#         'max_depth': trial.suggest_int('max_depth', 3, 15),\n#         'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n#         'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n#         'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),\n#         'n_estimators': 3000,\n#         'early_stopping_rounds': 50,\n#         'booster': 'gbtree',\n#         'tree_method': 'hist'\n        \n    }\n    \n    auc_scores = []\n    for train_idx, valid_idx in cv.split(X, y):\n        X_train_fold = X.iloc[train_idx]\n        X_valid_fold = X.iloc[valid_idx]\n        \n        y_xgb_train_fold = y_xgb.iloc[train_idx]\n        y_xgb_valid_fold = y_xgb.iloc[valid_idx]\n                \n        # Create and fit the model\n        model = XGBClassifier(**param)\n        model.fit(X_train_fold, y_xgb_train_fold, eval_set=[(X_valid_fold, y_xgb_valid_fold)],verbose=False)\n\n        # Predict class probabilities\n        y_prob = model.predict_proba(X_valid_fold)\n\n        # Compute the AUC for each class and take the average\n        average_auc = roc_auc_score(targets_bin.iloc[valid_idx], y_prob, multi_class=\"ovr\", average=\"macro\")\n        auc_scores.append(average_auc)\n\n    # Return the average AUC score across all folds\n    return np.mean(auc_scores)\n\n\nstudy = optuna.create_study(direction='maximize',study_name = \"xgb_model_training\")\nstudy.optimize(objective, n_trials=100)  # Adjust the number of trials as necessary\n# Output the optimization results\nprint(f\"Best trial average AUC: {study.best_value:.4f}\")\nprint(study.best_params)\nfor key, value in study.best_params.items():\n    print(f\"{key}: {value}\")/","metadata":{"execution":{"iopub.status.busy":"2024-06-20T07:42:15.402816Z","iopub.execute_input":"2024-06-20T07:42:15.403247Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"[I 2024-06-20 07:42:15,414] A new study created in memory with name: xgb_model_training\n[I 2024-06-20 07:42:58,511] Trial 0 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:43:40,694] Trial 1 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:44:22,900] Trial 2 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:45:05,174] Trial 3 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:45:47,666] Trial 4 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:46:28,454] Trial 5 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:47:09,273] Trial 6 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:47:49,874] Trial 7 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:48:31,467] Trial 8 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:49:13,153] Trial 9 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:49:54,804] Trial 10 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:50:35,309] Trial 11 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:51:15,744] Trial 12 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:51:56,191] Trial 13 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:52:38,669] Trial 14 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:53:19,079] Trial 15 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:53:59,512] Trial 16 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:54:41,561] Trial 17 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:55:23,323] Trial 18 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:56:03,976] Trial 19 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:56:46,244] Trial 20 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:57:28,105] Trial 21 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:58:09,953] Trial 22 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:58:52,258] Trial 23 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 07:59:34,267] Trial 24 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:00:15,987] Trial 25 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:00:56,578] Trial 26 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:01:38,339] Trial 27 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:02:18,815] Trial 28 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:03:00,760] Trial 29 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:03:42,660] Trial 30 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:04:24,345] Trial 31 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:05:06,146] Trial 32 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:05:48,278] Trial 33 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:06:30,100] Trial 34 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:07:10,712] Trial 35 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:07:52,624] Trial 36 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:08:33,249] Trial 37 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:09:15,018] Trial 38 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:09:56,790] Trial 39 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:10:37,434] Trial 40 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:11:19,398] Trial 41 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:12:01,692] Trial 42 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:12:43,572] Trial 43 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:13:25,598] Trial 44 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:14:06,435] Trial 45 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:14:48,224] Trial 46 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:15:28,743] Trial 47 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.8914799128399071.\n[I 2024-06-20 08:16:11,140] Trial 48 finished with value: 0.8914799128399071 and parameters: {'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8914799128399071.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### CATBoost","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    param = {\n        \"loss_function\": \"MultiClass\",\n        \"eval_metric\": \"MultiClass\",\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1),\n#         'n_estimators': 2000,\n#         \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 0.001, 10.0, log=True),\n#         \"depth\": trial.suggest_int(\"depth\", 4, 10),\n#         \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n#         \"bootstrap_type\": \"Bernoulli\",\n#         \"early_stopping_rounds\": 100,\n#         \"task_type\": 'CPU',\n   }\n\n    auc_scores = []\n\n    for train_idx, valid_idx in cv.split(X, y):\n        X_train_fold = X.iloc[train_idx]\n        X_valid_fold = X.iloc[valid_idx]\n        y_train_fold = y.iloc[train_idx]\n        y_valid_fold = y.iloc[valid_idx]\n\n        # Create and fit the model\n        model = CatBoostClassifier(**param)\n        model.fit(X_train_fold, y_train_fold, eval_set=[(X_valid_fold, y_valid_fold)])\n\n        # Predict class probabilities\n        y_prob = model.predict_proba(X_valid_fold)\n\n        # Compute the AUC for each class and take the average\n        average_auc = roc_auc_score(targets_bin.iloc[valid_idx], y_prob[:, 1:], multi_class=\"ovr\", average=\"macro\")\n        auc_scores.append(average_auc)\n\n    # Return the average AUC score across all folds\n    return np.mean(auc_scores)\n\n# Run Optuna optimization\ncatboost_study = optuna.create_study(direction='maximize', study_name=\"catboost_model_training\")\ncatboost_study.optimize(objective, n_trials=100)  # Adjust the number of trials as necessary\n\n# Output the optimization results\nprint(f\"Best average AUC: {study.best_value:.4f}\")\nprint(study.best_params)\nfor key, value in study.best_params.items():\n    print(f\"{key}: {value}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### LGBM","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    param = {\n    'objective': 'multiclass',  # Equivalent to multi:softmax but needs num_class as well\n    'num_class': 8,  # Specify the number of classes if your task is multi-class classification\n    'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n    'n_estimators': 3000,\n#     'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n#     'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n#     'max_depth': trial.suggest_int('max_depth', 3, 15),\n#     'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n#     'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n#     'min_child_weight': trial.suggest_int('min_child_weight', 1, 8),\n#     'device_type': 'cpu',\n#     'num_leaves': trial.suggest_int('num_leaves', 4, 2048),\n#     \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n#     \"verbosity\": -1,\n#     \"early_stopping_rounds\": 50,\n    }\n\n    auc_scores = []\n\n    for train_idx, valid_idx in cv.split(X, y):\n        X_train_fold = X.iloc[train_idx]\n        X_valid_fold = X.iloc[valid_idx]\n        y_train_fold = y.iloc[train_idx]\n        y_valid_fold = y.iloc[valid_idx]\n\n        # Create and fit the model\n        model = LGBMClassifier(**param)\n        model.fit(X_train_fold, y_train_fold, eval_set=[(X_valid_fold, y_valid_fold)], verbose=False)\n\n        # Predict class probabilities\n        y_prob = model.predict_proba(X_valid_fold)\n\n        # Compute the AUC for each class and take the average\n        average_auc = roc_auc_score(targets_bin.iloc[valid_idx], y_prob[:, 1:], multi_class=\"ovr\", average=\"macro\")\n        auc_scores.append(average_auc)\n\n    # Return the average AUC score across all folds\n    return np.mean(auc_scores)\n\nlgbm_study = optuna.create_study(direction='maximize',study_name = \"lgbm_model_training\")\nlgbm_study.optimize(objective, n_trials=100)  # Adjust the number of trials as necessary\n\n# Output the optimization results\nprint(f\"Best average AUC: {study.best_value:.4f}\")\nprint(study.best_params)\nfor key, value in study.best_params.items():\n    print(f\"{key}: {value}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### HGBM","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    param = {\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n        'max_iter': trial.suggest_int('max_iter', 100, 2500),  # Equivalent to n_estimators\n        'max_depth': trial.suggest_int('max_depth', 3, 20),\n        'l2_regularization': trial.suggest_float('l2_regularization', 1e-8, 10.0, log=True),  # Equivalent to reg_lambda\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 20, 300),\n        'max_bins': trial.suggest_int('max_bins', 25, 255),\n    }\n    \n    auc_scores = []\n\n    for train_idx, valid_idx in cv.split(X, y):\n        X_train_fold = X.iloc[train_idx]\n        X_valid_fold = X.iloc[valid_idx]\n        y_train_fold = y.iloc[train_idx]\n        y_valid_fold = y.iloc[valid_idx]\n        \n        # Create and fit the model\n        model = HistGradientBoostingClassifier(**param)\n        model.fit(X_train_fold, y_train_fold)\n\n        # Predict class probabilities\n        y_prob = model.predict_proba(X_valid_fold)\n\n        # Compute the AUC for each class and take the average\n        average_auc = roc_auc_score(targets_bin.iloc[valid_idx], y_prob[:, 1:], multi_class=\"ovr\", average=\"macro\")\n        auc_scores.append(average_auc)\n\n    # Return the average AUC score across all folds\n    return np.mean(auc_scores)\n\n\nhgbc_study = optuna.create_study(direction='maximize', study_name=\"HistGradientBoostingClassifier_model_training\")\nhgbc_study.optimize(objective, n_trials=100)  # Adjust the number of trials as necessary\n\n# Output the optimization results\nprint(f\"Best average AUC: {study.best_value:.4f}\")\nprint(study.best_params)\nfor key, value in study.best_params.items():\n    print(f\"{key}: {value}\")","metadata":{},"execution_count":null,"outputs":[]}]}